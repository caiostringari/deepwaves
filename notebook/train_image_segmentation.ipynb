{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train image segmentation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMz7xjMwc8Xo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kba7befUdJ8N"
      },
      "source": [
        "%tensorflow_version 2  # This tells Colab to use TF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JHZLhaXc9XO"
      },
      "source": [
        "Segment wave breaking pixels with UNet-line conv-nets.\n",
        "\n",
        "This program loads manually labbelled wave image data and classify\n",
        "each pixel in the image into \"breaking\" (1) or \"no-breaking\" (0).\n",
        "\n",
        "The data needs to be organized as follows:\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "└───train or test or valid\n",
        "    ├───images\n",
        "        ├───data\n",
        "               ├───img1.png\n",
        "               ├───img2.png\n",
        "               ...\n",
        "    ├───masks\n",
        "        ├───data\n",
        "               ├───img1.png\n",
        "               ├───img2.png\n",
        "               ...\n",
        "```\n",
        "\n",
        "The neural nets are modified UNets from:\n",
        "https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
        "https://www.tensorflow.org/tutorials/images/segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SnoLPpTdhYi"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeJpi_dkc1KO"
      },
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "import datetime\n",
        "\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tBrpb1mdzdo"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEqikctydV0D"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQRvNtQnd8m2"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/FEM/data/wave_breaking_detection/segmentation_v1.tar.gz\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYQSf4bceZEV"
      },
      "source": [
        "!tar -zxf segmentation_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRVg04zbes39"
      },
      "source": [
        "## Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3I5xJzwevKN"
      },
      "source": [
        "def xception(img_size, num_classes):\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    # -- [First half of the network: downsampling inputs] ---\n",
        "\n",
        "    # entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # --- [Second half of the network: upsampling inputs] ---\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(\n",
        "        num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GuIl6whexpH"
      },
      "source": [
        "# MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaHY_OWmedd3"
      },
      "source": [
        "def mobilenet(img_size, num_classes, model_path=\"mobilenet.h5\"):\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    # Use mobile net\n",
        "    # base_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3],\n",
        "    #                                                include_top=False)\n",
        "    base_model = load_model(model_path)\n",
        "\n",
        "    # use the activations of these layers\n",
        "    layer_names = [\n",
        "        'block_1_expand_relu',   # 64x64\n",
        "        'block_3_expand_relu',   # 32x32\n",
        "        'block_6_expand_relu',   # 16x16\n",
        "        'block_13_expand_relu',  # 8x8\n",
        "        'block_16_project']      # 4x4\n",
        "\n",
        "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    # create the feature extraction model\n",
        "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "    down_stack.trainable = False\n",
        "\n",
        "    # create the upstack\n",
        "    up_stack = [pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "                pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "                pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "                pix2pix.upsample(64, 3)]  # 32x32 -> 64x64\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "    x = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(x)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    # This is the last layer of the model\n",
        "    last = tf.keras.layers.Conv2DTranspose(num_classes, 3, strides=2,\n",
        "                                           padding='same')  # 64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3diYfpg5e4HC"
      },
      "source": [
        "def display_mask(val_preds):\n",
        "    \"\"\"Display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds, axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeFHtAmae4VT"
      },
      "source": [
        "def image_mask_generator(image_data_generator, mask_data_generator):\n",
        "    \"\"\"Yield a generator.\"\"\"\n",
        "    train_generator = zip(image_data_generator, mask_data_generator)\n",
        "    for (img, mask) in train_generator:\n",
        "        yield (img[0], mask[0][:, :, :, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TCNMmnwe8h3"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv5QJN5AfBpW"
      },
      "source": [
        "data = \"segmentation_v1\"\n",
        "backbone = \"xception\"\n",
        "pre_trained = None\n",
        "model_name = \"wave_xception\"\n",
        "img_size = (256, 256)\n",
        "batch_size = 32\n",
        "random_seed = 11\n",
        "epochs = 16\n",
        "logdir = \"logs\"\n",
        "learning_rate = 10E-6\n",
        "num_classes = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M41yl-effXvn"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ3QGPg1fSVB"
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "if platform.system().lower() == \"windows\":\n",
        "    logdir = logdir + \"\\\\\" + model_name + \"\\\\\" + date\n",
        "else:\n",
        "    logdir = logdir + \"/\" + model_name + \"/\" + date\n",
        "if not os.path.isdir(logdir):\n",
        "    os.makedirs(logdir, exist_ok=True)\n",
        "\n",
        "tensorboard = callbacks.TensorBoard(log_dir=logdir,\n",
        "                                    histogram_freq=1,\n",
        "                                    profile_batch=1)\n",
        "\n",
        "if platform.system().lower() == \"windows\":\n",
        "    checkpoint_path = logdir + \"\\\\\" + \"best.h5\"\n",
        "else:\n",
        "    checkpoint_path = logdir + \"/\" + \"best.h5\"\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                        save_best_only=True,\n",
        "                                        save_weights_only=False,\n",
        "                                        monitor='val_loss',\n",
        "                                        mode=\"min\",\n",
        "                                        verbose=1)\n",
        "if platform.system().lower() == \"windows\":\n",
        "    pred_out = logdir + \"\\\\\" + \"pred\"\n",
        "else:\n",
        "    pred_out = logdir + \"/\" + \"pred\"\n",
        "os.makedirs(pred_out, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLNujsSUfcwb"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rHAKs1AfmVh"
      },
      "source": [
        "# train generators - they need to be identical!\n",
        "image_train_generator = ImageDataGenerator(\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/train/images\",\n",
        "                                           batch_size=batch_size,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "mask_train_generator = ImageDataGenerator(\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    rotation_range=10,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/train/masks\",\n",
        "                                           batch_size=batch_size,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "\n",
        "# test/valid generators - they need to be identical!\n",
        "# note that no augmentation is really done to this data, only a resize\n",
        "image_valid_generator = ImageDataGenerator(\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/valid/images\",\n",
        "                                           batch_size=batch_size,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "\n",
        "mask_valid_generator = ImageDataGenerator(\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/valid/masks\",\n",
        "                                           batch_size=batch_size,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "image_test_generator = ImageDataGenerator(\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/test/images\",\n",
        "                                           batch_size=1,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "\n",
        "mask_test_generator = ImageDataGenerator(\n",
        "    rescale=1. / 255.).flow_from_directory(data + \"/test/masks\",\n",
        "                                           batch_size=1,\n",
        "                                           target_size=img_size,\n",
        "                                           seed=random_seed)\n",
        "\n",
        "train_generator = image_mask_generator(image_train_generator, mask_train_generator)\n",
        "valid_generator = image_mask_generator(image_valid_generator, mask_valid_generator)\n",
        "test_generator = image_mask_generator(image_test_generator, mask_test_generator)\n",
        "train_size = image_train_generator.n\n",
        "valid_size = image_valid_generator.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3q4Zt1GgBV5"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnNvVGyxgA7m"
      },
      "source": [
        "# define the model\n",
        "if backbone.lower() == \"xception\":\n",
        "    model = xception(img_size, num_classes)\n",
        "elif backbone.lower() == \"mobilenet\":\n",
        "    if not pre_trained:\n",
        "        raise ValueError(\"Pre-trained model is required with {}.\".format(backbone))\n",
        "    model = mobilenet(img_size, num_classes, pre_trained)\n",
        "else:\n",
        "    raise NotImplementedError(\"Backbone {} is not implemented\".format(backbone))\n",
        "model.summary()\n",
        "\n",
        "# configure the model for training.\n",
        "# we use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data are integers.\n",
        "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model, doing validation at the end of each epoch.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch=train_size // batch_size,\n",
        "                    validation_data=valid_generator,\n",
        "                    validation_steps=valid_size // batch_size,\n",
        "                    callbacks=[tensorboard, checkpoint])\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist[\"epoch\"] = history.epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D56hlqdngKdi"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5q-aRYIgLYv"
      },
      "source": [
        "# predict on the test data and save the outputs\n",
        "for i in range(4):\n",
        "    img, msk = next(test_generator)\n",
        "    val_preds = model.predict(img)\n",
        "    # process prediction\n",
        "    prd = display_mask(val_preds)\n",
        "    # plot\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6),\n",
        "                                        sharex=True, sharey=True)\n",
        "    ax1.imshow(np.squeeze(img))\n",
        "    ax2.imshow(np.squeeze(msk))\n",
        "    ax3.imshow(np.squeeze(prd))\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhS3I3MxgaMN"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBwPl6l6gckx"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}